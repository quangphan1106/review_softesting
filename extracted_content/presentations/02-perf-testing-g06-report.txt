
--- Page 1 ---
Performance Testing Report
CS423 - Software Testing
Group 06
LÃª Thanh LÃ¢m - 22125046
Nguyá»…n HoÃ ng PhÃºc - 22125076
Nguyá»…n ChÃ­nh ThÃ´ng - 22125102
Nguyá»…n VÃµ HoÃ ng ThÃ´ng - 22125103
Contents
1 Â· Introduction ................................................................................ 1
1.1 Â· Overview ............................................................................. 1
1.2 Â· Importance ........................................................................... 2
2 Â· Theoretical Background ..................................................................... 2
2.1 Â· Key Concepts and Metrics ............................................................. 2
2.2 Â· Types of Performance Testing ......................................................... 2
2.3 Â· Performance Testing Lifecycle ......................................................... 2
2.4 Â· Workload Modeling .................................................................... 3
2.5 Â· Tools and Framework Context (Preview) ............................................... 3
3 Â· Performance Testing Methodology ........................................................... 3
3.1 Â· Step-by-Step Process ................................................................. 3
3.2 Â· Common Performance Bottlenecks .................................................... 3
3.3 Â· Best Practices ......................................................................... 4
4 Â· Tools & Frameworks ........................................................................ 4
4.1 Â· Classification of Performance Testing Tools ............................................. 4
4.2 Â· Popular Performance Testing Tools ..................................................... 6
4.3 Â· Tool Selection Criteria ................................................................. 7
4.4 Â· Modern Trends ........................................................................ 7
5 Â· Tools & Setup - Apache JMeter .............................................................. 7
5.1 Â· What JMeter is ........................................................................ 7
5.2 Â· Installing and Preparing JMeter ........................................................ 7
5.3 Â· Core JMeter Components .............................................................. 7
5.4 Â· JMeter Plugins and Extensions (Brief) ................................................. 8
5.5 Â· Distributed Testing (Scaling Load) ..................................................... 8
6 Â· Practical Demonstration - Example Test Plan ................................................ 8
6.1 Â· Example Test Scenario (Choose One for Demo) ........................................ 8
6.2 Â· Test Plan Structure (High Level) ....................................................... 8
6.3 Â· Writing Realistic Workloads ............................................................ 8
6.4 Â· Example: Run JMeter Non-GUI and Generate HTML Dashboard ......................... 9
6.5 Â· What the HTML Dashboard Shows ..................................................... 9
7 Â· Related Work ............................................................................... 9
1 Â· Introduction
1.1 Â· Overview
Performance testing is a type of non-functional testing  focused on evaluating how a system 
behaves under specific workloads. Unlike functional testing, which verifies correctness, performance 
testing measures responsiveness, stability, scalability, and resource usage.
Key performance attributes include:
â€¢ Response time â€“ How quickly a system responds to a request.
â€¢ Throughput â€“ The number of transactions processed per second.
â€¢ Resource utilization â€“ CPU, memory, disk, and network usage.
â€¢ Scalability â€“ How well the system handles increasing load.
Performance testing ensures that software:
1
--- Page 2 ---
Performance Testing Report Group 06
â€¢ Meets service-level agreements (SLAs).
â€¢ Can scale under expected user loads.
â€¢ Identifies performance bottlenecks before deployment.
1.2 Â· Importance
Performance testing is crucial for:
â€¢ User satisfaction â€“ Slow systems degrade user experience.
â€¢ System reliability â€“ Detects failures under high concurrency.
â€¢ Cost optimization â€“ Avoids over-provisioning or inefficient scaling.
â€¢ Business continuity â€“ Prevents outages during peak usage.
2 Â· Theoretical Background
2.1 Â· Key Concepts and Metrics
Term Definition
Response Time (RT) Time between request submission and response completion.
Throughput (TP) Number of transactions or requests handled per second.
Latency (L) Delay before a response begins.
Concurrency Number of simultaneous users or sessions.
Error Rate Percentage of failed transactions.
Resource Utilization CPU, memory, network, or I/O usage during test execution.
2.1.1 Â· Relationship between metrics
The fundamental relationship between throughput and response time is:
Throughput = Number of Requests
Total Time
Response time directly influences perceived performance.
2.2 Â· Types of Performance Testing
Type Purpose Example Scenario
Load Testing Measure system behavior under 
expected user load.
100 concurrent users accessing 
a website.
Stress Testing Determine system limits by in -
creasing load beyond capacity.
Simulate 10,000 users until fail -
ure.
Spike Testing Evaluate systemâ€™s recovery from 
sudden load spikes.
Sudden traffic surge during a 
product launch.
Endurance (Soak) Testing Assess stability over long dura -
tions.
Continuous API calls for 24 
hours.
Scalability Testing Measure performance while 
scaling resources.
Increasing number of nodes in a 
cluster.
2.3 Â· Performance Testing Lifecycle
1. Requirement Gathering â€“ Identify performance goals (e.g., â€œhandle 500 concurrent users with 
< 2s response timeâ€).
2. Planning â€“ Define workload model, test scenarios, and acceptance criteria.
3. Design & Implementation â€“ Develop test scripts (e.g., with JMeter or k6).
4. Execution â€“ Run tests in a controlled environment.
5. Monitoring & Analysis â€“ Record system metrics and interpret results.
6. Reporting & Optimization â€“ Identify bottlenecks and recommend improvements.
2
--- Page 3 ---
Performance Testing Report Group 06
â€œPerformance testing is iterative: each test cycle feeds insights into performance tuning and archi -
tecture refinement.â€ â€” Jain, R. (1991)
2.4 Â· Workload Modeling
Workload modeling involves simulating real user behavior:
â€¢ Define user profiles (types of users or transactions).
â€¢ Establish think time between user actions.
â€¢ Distribute requests per second (RPS) and concurrency.
Mathematically, a simple workload can be modeled as:
ð‘Š = ð‘ Ã— (ð‘…
ð‘‡ )
where N = number of users, R = requests per user, T = total test duration.
2.5 Â· Tools and Framework Context (Preview)
While tools are discussed later, understanding their role helps in theory:
â€¢ Apache JMeter â€“ simulates HTTP requests, measures throughput and latency.
â€¢ k6 â€“ modern load-testing tool using JavaScript scripting.
â€¢ Locust â€“ Python-based, user-oriented load testing.
3 Â· Performance Testing Methodology
3.1 Â· Step-by-Step Process
1. Define Performance Goals
â€¢ Examples: â€œAverage response time â‰¤ 2sâ€, â€œError rate â‰¤ 1%â€, â€œCPU â‰¤ 70% utilizationâ€.
â€¢ Goals should be measurable and traceable to business needs.
2. Identify Test Scenarios
â€¢ Select critical user flows (login, checkout, API endpoint).
â€¢ Prioritize based on system usage frequency and risk.
3. Design Test Scripts
â€¢ Simulate virtual users executing real workloads.
â€¢ Parameterize requests (dynamic data, sessions).
â€¢ Include assertions for success criteria.
4. Set Up Test Environment
â€¢ Isolate performance environment from production.
â€¢ Ensure representative hardware, network, and databases.
â€¢ Use monitoring tools (Grafana, Prometheus, New Relic).
5. Execute and Monitor Tests
â€¢ Gradually increase load to observe threshold and breaking points.
â€¢ Collect metrics: latency, throughput, CPU/memory, network I/O.
â€¢ Observe logs and error responses.
6. Analyze and Report Results
â€¢ Compare metrics against baseline and goals.
â€¢ Identify bottlenecks (e.g., slow DB queries, memory leaks).
â€¢ Visualize results via charts (JMeter HTML report, Grafana dashboards).
7. Optimization and Retesting
â€¢ Tune configurations (e.g., caching, DB indexing, thread pools).
â€¢ Retest after each optimization to validate improvements.
3.2 Â· Common Performance Bottlenecks
â€¢ Database issues: inefficient queries, missing indexes.
â€¢ Server-side logic: blocking I/O, excessive thread contention.
â€¢ Network latency: bandwidth constraints, high round-trip times.
3
--- Page 4 ---
Performance Testing Report Group 06
â€¢ Frontend performance: heavy assets, client-side rendering delays.
3.3 Â· Best Practices
â€¢ Test early and continuously (shift-left performance testing).
â€¢ Keep test data realistic and production-like.
â€¢ Correlate metrics with business KPIs.
â€¢ Automate performance tests in CI/CD pipelines.
â€¢ Use statistical methods (percentiles, averages, standard deviation) for analysis.
4 Â· Tools & Frameworks
Performance testing tools and frameworks have evolved significantly to meet diverse testing require-
ments. Understanding their characteristics and classifications helps teams select the most appropriate 
tools for their specific needs.
4.1 Â· Classification of Performance Testing Tools
Performance testing tools can be categorized along several dimensions, each offering different 
advantages and trade-offs:
4.1.1 Â· Interface Type: Script-based vs. UI-based
Type Strengths Weaknesses Examples
Script-based â€¢ Full flexibility and 
control
â€¢ Version control 
friendly (Git)
â€¢ Easy CI/CD integra -
tion
â€¢ Reusable code com -
ponents
â€¢ Advanced cus -
tomization
â€¢ Better code review 
process
â€¢ Steeper learning 
curve
â€¢ Requires program -
ming skills
â€¢ Slower initial setup
â€¢ Less intuitive for be-
ginners
â€¢ Debugging can be 
complex
Locust, K6, Gatling, Artillery
UI-based â€¢ Low barrier to entry
â€¢ Quick test creation
â€¢ Visual workflow de -
sign
â€¢ Record and play -
back
â€¢ Easy for non-pro -
grammers
â€¢ Immediate visual 
feedback
â€¢ Less flexible for 
complex scenarios
â€¢ Harder to version 
control
â€¢ Limited code 
reusability
â€¢ UI can be resource-
intensive
â€¢ May still need script-
ing for advanced 
cases
JMeter, LoadRunner, Ne -
oLoad
4.1.2 Â· Deployment Model: Local vs. Cloud-based
Type Strengths Weaknesses Examples
Local â€¢ Full control over envi -
ronment
â€¢ No recurring cloud 
costs
â€¢ Data stays on-
premise (security)
â€¢ Limited by local re -
sources
â€¢ Cannot simulate 
global users easily
â€¢ Infrastructure main -
tenance burden
JMeter, Locust, Gatling, 
K6 (OSS)
4
--- Page 5 ---
Performance Testing Report Group 06
Type Strengths Weaknesses Examples
â€¢ No internet depen -
dency
â€¢ Predictable perfor -
mance
â€¢ One-time setup cost
â€¢ Scaling requires hard-
ware investment
â€¢ Single point of failure
Cloud-based â€¢ Massive scalability 
on-demand
â€¢ Geographic distribu -
tion
â€¢ No infrastructure 
management
â€¢ Pay-as-you-go model
â€¢ Rapid test execution
â€¢ Built-in monitoring/
reporting
â€¢ Recurring subscrip -
tion costs
â€¢ Data leaves premises 
(compliance issues)
â€¢ Internet dependency
â€¢ Vendor lock-in risks
â€¢ Potential latency is -
sues
â€¢ Less control over in -
frastructure
BlazeMeter, K6 Cloud, 
Loader.io, AWS Device 
Farm
Hybrid â€¢ Best of both worlds
â€¢ Gradual cloud migra -
tion
â€¢ Flexible resource allo -
cation
â€¢ Cost optimization op -
tions
â€¢ Local dev, cloud pro -
duction testing
â€¢ Complex setup and 
management
â€¢ Requires expertise in 
both models
â€¢ Potential sync issues
â€¢ Higher overall com -
plexity
K6, Gatling Enterprise, 
JMeter + Cloud
4.1.3 Â· Programming Language & Ecosystem
Language Tools Strengths Weaknesses
Python Locust, Molotov Easy to learn, rich libraries, 
great for data processing, 
readable syntax, wide adop -
tion
Slower execution, GIL limita -
tions, higher memory usage
JavaScript K6, Artillery, Play -
wright
Web-native, async/await 
support, large ecosystem, 
modern tooling, Node.js per -
formance
Callback complexity, less 
suitable for CPU-intensive 
tasks, runtime quirks
Java JMeter, Gatling Enterprise-grade, robust, ex -
cellent performance, mature 
ecosystem, strong typing
Verbose syntax, slower 
startup, higher resource us -
age, steep learning curve
Go Vegeta, Hey Excellent performance, low 
resource usage, built-in con -
currency, fast compilation
Smaller ecosystem, less li -
braries, simpler language 
features, smaller community
Ruby Tsung Rails integration, elegant 
syntax, developer-friendly, 
quick prototyping
Performance limitations, 
smaller performance testing 
community, less tooling
4.1.4 Â· Protocol Support
Category Strengths Weaknesses Examples
Multi-protocol â€¢ Single tool for diverse 
protocols
â€¢ Comprehensive testing 
capability
â€¢ Complexity overhead
â€¢ Steeper learning curve
â€¢ May not excel at any 
single protocol
JMeter, LoadRunner
5
--- Page 6 ---
Performance Testing Report Group 06
Category Strengths Weaknesses Examples
â€¢ Reusable infrastructure
â€¢ Covers legacy systems
â€¢ Standardized approach
â€¢ Larger footprint
â€¢ Plugin dependency
HTTP-focused â€¢ Optimized for web/API
â€¢ Modern features 
(HTTP/2, WebSocket)
â€¢ Better performance for 
HTTP
â€¢ Simpler, cleaner API
â€¢ Faster execution
â€¢ Limited to HTTP-based 
protocols
â€¢ Cannot test databases 
directly
â€¢ Not suitable for legacy 
systems
â€¢ May need multiple tools
K6, Locust, Artillery
Specialized â€¢ Domain expertise
â€¢ Highly optimized
â€¢ Specific metrics
â€¢ Deep protocol knowl -
edge
â€¢ Accurate simulation
â€¢ Limited scope
â€¢ Additional tools needed
â€¢ Smaller communities
â€¢ Less general applicabil -
ity
â€¢ Niche use cases only
HammerDB, JMeter 
plugins
4.2 Â· Popular Performance Testing Tools
4.2.1 Â· Open Source Tools
Locust
â€¢ Python-based, script-driven
â€¢ Distributed load generation
â€¢ Real-time web UI for monitoring
â€¢ Event-driven, can simulate millions of users
â€¢ Best for: Teams comfortable with Python, API testing
K6
â€¢ JavaScript-based, modern developer experience
â€¢ Built-in metrics and thresholds
â€¢ Cloud and local execution
â€¢ Excellent CI/CD integration
â€¢ Best for: DevOps teams, modern web applications
JMeter
â€¢ Java-based, industry standard
â€¢ Both GUI and CLI modes
â€¢ Extensive protocol support and plugins
â€¢ Large community and resources
â€¢ Best for: Enterprise environments, complex protocols
Gatling
â€¢ Scala-based, code-as-configuration
â€¢ High-performance engine
â€¢ Detailed HTML reports
â€¢ Strong IDE support
â€¢ Best for: High-throughput testing, Scala/Java teams
Artillery
â€¢ JavaScript/Node.js based
â€¢ YAML configuration with JS extensions
â€¢ Serverless testing support
â€¢ Quick setup and execution
â€¢ Best for: Quick tests, microservices
6
--- Page 7 ---
Performance Testing Report Group 06
4.2.2 Â· Commercial Tools
Tool Key Features Best For
LoadRunner Enterprise-grade, comprehensive protocols, ad -
vanced analytics
Large enterprises, complex ap -
plications
NeoLoad Codeless design, APM integration, dynamic infra -
structure
Agile teams, SAP testing
BlazeMeter JMeter-compatible, cloud-based, geo-distributed 
testing
Teams using JMeter wanting 
cloud scale
K6 Cloud K6 script compatible, performance insights, team 
collaboration
K6 users needing cloud features
4.3 Â· Tool Selection Criteria
When selecting a performance testing tool, consider:
1. Technical Requirements: Protocols needed, application architecture, integration points
2. Team Skills: Programming language familiarity, learning curve
3. Budget: Open source vs. commercial licensing costs
4. Scale: Expected load levels, geographic distribution needs
5. CI/CD Integration: Automation capabilities, reporting formats
6. Reporting & Analysis: Visualization needs, stakeholder requirements
7. Community & Support: Documentation quality, available resources
4.4 Â· Modern Trends
The performance testing landscape is evolving with several key trends:
â€¢ Shift-left Testing: Earlier performance testing in development cycle
â€¢ Cloud-native: Kubernetes-aware testing, containerized test execution
â€¢ Observability Integration: Correlation with APM, distributed tracing
â€¢ AI/ML: Intelligent test generation, anomaly detection
â€¢ Developer-centric: Code-based tests, version control, modern languages
â€¢ Continuous Performance: Automated performance gates in CI/CD pipelines
5 Â· Tools & Setup - Apache JMeter
5.1 Â· What JMeter is
Apache JMeter is a 100% Java open-source application for load and performance testing. It simu -
lates multiple users (threads) sending requests to a server and collects response metrics (latency, 
throughput, errors).
5.2 Â· Installing and Preparing JMeter
â€¢ Prerequisite: Java JDK (use the latest LTS supported by JMeter).
â€¢ Download: Official binary from the Apache JMeter site; unzip and run bin/jmeter (GUI) or bin/
jmeter.bat on Windows.
Important JVM / OS recommendations (practical):
â€¢ Increase JMeter JVM heap if you plan many threads or large listeners (edit JVM_ARGS in bin/jmeter or 
jmeter.bat, e.g. -Xms1g -Xmx4g). The user manual and best-practices strongly encourage tuning the 
JVM for heavy tests.
â€¢ Do not run large load tests in GUI mode - use non-GUI mode for execution to avoid the GUI 
overhead. The manual and best-practices emphasize non-GUI execution for real load runs.
5.3 Â· Core JMeter Components
â€¢ Test Plan - root of a JMeter test suite.
â€¢ Thread Group - defines virtual users, ramp-up, loop count.
7
--- Page 8 ---
Performance Testing Report Group 06
â€¢ Samplers - HTTP Request, JDBC Request, etc. (what actions virtual users perform).
â€¢ Logic Controllers - control flow (If Controller, Loop Controller, Transaction Controller).
â€¢ Timers - add think time (Uniform Random Timer, Constant Timer). Realistic workloads require 
think time.
â€¢ Assertions - verify responses (status code, JSON path, size). Useful to ensure validity under load.
â€¢ Config Elements  - HTTP Header Manager, CSV Data Set config (parameterization), Cookie 
Manager.
â€¢ Listeners - View Results Tree, Aggregate Report, Summary Report - used for debugging and result 
export. For heavy runs, minimize GUI listeners and instead write results to disk for post-processing.
5.4 Â· JMeter Plugins and Extensions (Brief)
JMeter has a plugin ecosystem (e.g., JMeter Plugins Manager) that adds advanced samplers, timers, 
and graphs. Plugins are useful but always validate their stability for your test environment (stick to 
essential plugins for reproducibility).
5.5 Â· Distributed Testing (Scaling Load)
If a single machine cannot generate required load, JMeter supports remote (distributed) testing 
- one client (controller) drives multiple remote JMeter servers (agents) to multiply the virtual user 
capacity. Follow the official remote testing steps and RMI configuration; ensure network/firewall and 
identical test plan & plugin versions on all nodes.
6 Â· Practical Demonstration - Example Test Plan
6.1 Â· Example Test Scenario (Choose One for Demo)
Target: A RESTful API https://api.example.com/v1/items - typical CRUD flows.
User flows to test (prioritized):
1. Browse items (GET /v1/items?page=X) - 60% of traffic
2. Get item details (GET /v1/items/{id}) - 30%
3. Create item (POST /v1/items) - 10% (requires authentication + dynamic data)
6.2 Â· Test Plan Structure (High Level)
â€¢ Test Plan:
â€£ Thread Group (Virtual Users: configurable)
â€“ HTTP Request Defaults (server = api.example.com, protocol = https)
â€“ CSV Data Set Config (for dynamic payloads / item IDs)
â€“ Cookie Manager / Authorization Manager (if auth required)
â€“ Throughput Controller / Weighted Switch (to shape traffic mix)
â€“ Samplers:
â€¢ HTTP Request (GET /v1/items)
â€¢ HTTP Request (GET /v1/items/${id})
â€¢ HTTP Request (POST /v1/items) with Body Data (from CSV)
â€“ Timers (think time between requests)
â€“ Assertions (HTTP Code = 200/201, JSON verify)
â€“ Backend Listener (optional - send metrics to external metrics backend)
â€¢ Result Writer: configure Simple Data Writer or set -l log file on CLI (CSV or JTL).
Tip: keep one â€œmasterâ€ Thread Group for main scenario and separate Thread Groups for background 
traffic (e.g., scheduled batch jobs), if needed.
6.3 Â· Writing Realistic Workloads
â€¢ Use CSV Data Set Config to parameterize payloads and IDs so each virtual user simulates distinct 
users/data.
â€¢ Use timers to emulate real think time (do not hammer with constant 0ms between requests unless 
testing raw capacity).
8
--- Page 9 ---
Performance Testing Report Group 06
â€¢ Model concurrency appropriately: ramp-up should be gradual (e.g., ramp threads over 60â€“300s 
depending on goals) to observe warm-up behavior.
6.4 Â· Example: Run JMeter Non-GUI and Generate HTML Dashboard
1. Execute (non-GUI):
jmeter -n -t my_test_plan.jmx -l results.jtl -e -o /tmp/jmeter-report
â€¢ -n: non-GUI
â€¢ -t: test plan file
â€¢ -l: results log (JTL)
â€¢ -e -o: generate the HTML report to specified output folder.
This is the official recommended way to run and produce the dashboard post-run.
2. Generate report from existing log (if you already have a .jtl):
jmeter -g results.jtl -o /tmp/jmeter-report
This regenerates the HTML dashboard from a previous log file.
3. When running distributed: start jmeter-server on remote agents and use the master to run the 
same command; consult the distributed testing step-by-step guide for firewall/RMI details.
6.5 Â· What the HTML Dashboard Shows
The JMeter dashboard contains multiple charts and tables; important items to inspect:
â€¢ Requests per second (throughput) - checks handling capacity.
â€¢ Response times (median, 90th, 95th, max) - percentiles are more actionable than averages.
â€¢ Errors / error types - HTTP 4xx/5xx breakdown and error messages.
â€¢ Active threads over time - verify concurrency was injected as planned.
â€¢ Apdex / satisfaction (if configured) - mapping response time thresholds to user experience.
Use percentiles (p90/p95/p99) to characterize tail latency, averages hide spikes.
7 Â· Related Work
This performance testing report builds on foundational concepts and methodologies from software 
testing literature and practical industry best practices.
9